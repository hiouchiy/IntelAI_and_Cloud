{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Welcome to Jupyter Notebooks on the Intel AI DevCloud! \n",
    "This document covers the basics of the Jupyter Notebook access to the Intel AI DevCloud. It is not a tutorial on the Jupyter Notebook itself. Rather, we will run through a few examples of how to use the computational resources available on the DevCloud *beyond* the notebook.\n",
    "\n",
    "The diagram below illustrates the high-level organization of the DevCloud. This tutorial explains how to navigate this organization. \n",
    "\n",
    "<img src=\"https://access.colfaxresearch.com/images/cluster-jn-organization.svg\" style=\"max-width:600px;\" />\n",
    "\n",
    "\n",
    "## Service Terms\n",
    "\n",
    "By using the Intel AI DevCloud, you are agreeing to the following Service Terms: <br />\n",
    "<a href=\"https://access.colfaxresearch.com/doc/Colfax_Cluster_Service_Terms.pdf\">https://access.colfaxresearch.com/doc/Colfax_Cluster_Service_Terms.pdf</a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. 画像データをダウンロードする"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://cs298395642e8d6x4498x8b7.blob.core.windows.net/share/data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv data.zip train_data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip train_data/data.zip -d train_data/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. モデルファイルをOpenVINOのIR形式に変換する"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "事前にモデルファイル「model.pb」をJupyter Notebookにアップロードする必要があります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/opt/intel/openvino/deployment_tools/model_optimizer/mo.py \\\n",
    "--input_model model.pb \\\n",
    "--data_type FP32 \\\n",
    "--output_dir . \\\n",
    "--input_shape [1,224,224,3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. バックエンドにてIRを実行するPythonスクリプトを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_inference.py\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.mobilenet import preprocess_input\n",
    "from tensorflow.keras import models\n",
    "import numpy as np\n",
    "import glob\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import sys\n",
    "\n",
    "from openvino.inference_engine import IENetwork, IEPlugin\n",
    "\n",
    "def inference_openvino(total = 100, target_device=\"CPU\"):\n",
    "    model_xml = 'model.xml'\n",
    "    model_bin = 'model.bin'\n",
    "\n",
    "    # Plugin initialization for specified device and load extensions library if specified\n",
    "    # Set the desired device name as 'device' parameter. This sample support these 3 names: CPU, GPU, MYRIAD\n",
    "    ie = IEPlugin(device=target_device, plugin_dirs='')\n",
    "\n",
    "    # Read IR\n",
    "    net = IENetwork(model=model_xml, weights=model_bin)\n",
    "\n",
    "    input_blob = next(iter(net.inputs))\n",
    "    out_blob = next(iter(net.outputs))\n",
    "    net.batch_size = 1\n",
    "\n",
    "    # Loading model to the plugin\n",
    "    exec_net = ie.load(network=net)\n",
    "    \n",
    "    list_df = pd.DataFrame( columns=['正解ラベル','予測ラベル','全処理時間(msec)','推論時間(msec)'] )\n",
    "\n",
    "    total_spent_time = 0\n",
    "    total_infer_spent_time = 0\n",
    "    \n",
    "    for j in range(total):\n",
    "        time1 = time.time()\n",
    "        file_list = glob.glob(\"train_data/test/*/*\")\n",
    "        img_path = random.choice(file_list)\n",
    "        img_cat = os.path.split(os.path.dirname(img_path))[1]\n",
    "        # Read and pre-process input images\n",
    "        n, c, h, w = net.inputs[input_blob].shape\n",
    "        images = np.ndarray(shape=(n, c, h, w))\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        if image.shape[:-1] != (h, w):\n",
    "            image = cv2.resize(image, (w, h))\n",
    "        frame = image\n",
    "        image = image.transpose((2, 0, 1))  # Change data layout from HWC to CHW\n",
    "        image = image.reshape((n, c, h, w))\n",
    "        #image = preprocess_input(image)\n",
    "        images[0] = image\n",
    "\n",
    "        # Start sync inference\n",
    "        time2 = time.time()\n",
    "        preds = exec_net.infer(inputs={input_blob: images})\n",
    "        \n",
    "        infer_spent_time = time.time() - time2\n",
    "        total_infer_spent_time += infer_spent_time\n",
    "        \n",
    "        spent_time = time.time() - time1\n",
    "        total_spent_time += spent_time\n",
    "        \n",
    "        preds = preds[out_blob]\n",
    "        top = preds[0].argsort()[-1:][::-1]\n",
    "        #pred_label = labels[top[0]]\n",
    "        pred_label = top[0]\n",
    "        tmp_se = pd.Series( [img_cat, pred_label, str(int(spent_time * 1000)), str(int(infer_spent_time * 1000)) ], index=list_df.columns )\n",
    "        list_df = list_df.append( tmp_se, ignore_index=True ) \n",
    "\n",
    "    print()\n",
    "    print('全' + str(total) + '枚 完了！')\n",
    "    print()\n",
    "    print(\"平均処理時間: \" + str(int((total_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(\"平均推論時間: \" + str(int((total_infer_spent_time / total)*1000.0)) + \" ms/枚\")\n",
    "    print(list_df)\n",
    "    \n",
    "    return int((total_spent_time / total)*1000.0), int((total_infer_spent_time / total)*1000.0)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    cpu_total_time, cpu_infer_time = inference_openvino(total=50, target_device=\"CPU\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. バックエンドで動作するShell Scriptを作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run.sh\n",
    "cd $PBS_O_WORKDIR\n",
    "echo \"* Hello world from compute server `hostname`!\"\n",
    "echo \"* The current directory is ${PWD}.\"\n",
    "echo \"* Compute server's CPU model and number of logical CPUs:\"\n",
    "lscpu\n",
    "echo \"* Python available to us:\"\n",
    "which python\n",
    "python --version\n",
    "\n",
    "#Setup related environment variables\n",
    "source /opt/intel/openvino/bin/setupvars.sh\n",
    "\n",
    "#Install related python libraries\n",
    "pip install networkx defusedxml protobuf intel-tensorflow==1.15.0 pillow --user\n",
    "\n",
    "#Run python script in parallel\n",
    "numactl -N 0 -m 0 python object_detection_demo_ssd_async.py &\n",
    "numactl -N 1 -m 1 python object_detection_demo_ssd_async.py &\n",
    "\n",
    "wait\n",
    "echo \"*Bye\"\n",
    "# Remember to have an empty line at the end of the file; otherwise the last command will not run\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Job Submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Skylakeを使う場合はこちら\n",
    "!qsub run.sh -l nodes=1:fpga_compile:ppn=2\n",
    "\n",
    "#Cascade Lakeを使う場合はこちら\n",
    "#!qsub run.sh -l nodes=1:experimental:ppn=2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JobのStatusは下記コマンドにて確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Jobの結果を確認"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "標準出力はoから始まる名前のファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat run.sh.o*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "エラー出力はeから始まる名前のファイル"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cat run.sh.e*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以降はおまけ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"* How many compute servers are available?\"\n",
    "!pbsnodes | grep \"^c\" | wc -l\n",
    "\n",
    "!echo \"* How many of them are free?\"\n",
    "!pbsnodes | grep \"state = free\" | wc -l\n",
    "\n",
    "!echo \"* What are the time limits for queued jobs?\"\n",
    "!qmgr -c 'p q batch' | grep walltime\n",
    "\n",
    "!echo \"* What is the configuration of the available compute servers?\"\n",
    "!pbsnodes | grep properties | sort | uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Intel, 2019 update 2)",
   "language": "python",
   "name": "c009-intel_distribution_of_python_3_2019u2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
