{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 $INTEL_OPENVINO_DIR/deployment_tools/tools/model_downloader/downloader.py --list models.lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x build-poseextractor.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./build-poseextractor.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "from openvino.inference_engine import IENetwork, IECore\n",
    "import os\n",
    "import logging as log\n",
    "import sys\n",
    "from PIL import Image\n",
    "import PIL\n",
    "import io\n",
    "import IPython.display\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# C++ module for extracting pose from PAFs and heatmaps\n",
    "from pose_extractor import extract_poses\n",
    "\n",
    "limbIds = [\n",
    "        [ 1,  2], [ 1,  5], [ 2,  3], [ 3,  4], [ 5,  6], [ 6,  7], [ 1,  8], [ 8,  9], [ 9, 10], [ 1, 11],\n",
    "        [11, 12], [12, 13], [ 1,  0], [ 0, 14], [14, 16], [ 0, 15], [15, 17], [ 2, 16], [ 5, 17] ]\n",
    "\n",
    "limbColors = [\n",
    "    (255,  0,  0), (255, 85,  0), (255,170,  0),\n",
    "    (255,255,  0), (170,255,  0), ( 85,255,  0),\n",
    "    (  0,255,  0), (  0,255, 85), (  0,255,170),\n",
    "    (  0,255,255), (  0,170,255), (  0, 85,255),\n",
    "    (  0,  0,255), ( 85,  0,255), (170,  0,255),\n",
    "    (255,  0,255), (255,  0,170), (255,  0, 85)\n",
    "]\n",
    "\n",
    "def renderPeople(img, people, scaleFactor=4, threshold=0.5):\n",
    "    global limbIDs\n",
    "    global limbColors\n",
    "    # 57x32 = resolution of HM and PAF\n",
    "    scalex = img.shape[1]/(57 * scaleFactor)\n",
    "    scaley = img.shape[0]/(32 * scaleFactor)\n",
    "    for person in people:\n",
    "        for i, limbId in enumerate(limbIds[:-2]):\n",
    "            x1, y1, conf1 = person[limbId[0]*3:limbId[0]*3+2 +1]\n",
    "            x2, y2, conf2 = person[limbId[1]*3:limbId[1]*3+2 +1]\n",
    "            if conf1>threshold and conf2>threshold:\n",
    "                cv2.line(img, (int(x1*scalex),int(y1*scaley)), (int(x2*scalex),int(y2*scaley)), limbColors[i], 2)\n",
    "\n",
    "\n",
    "def main(video_path):\n",
    "\n",
    "    # Prep for OpenVINO Inference Engine for human pose estimation\n",
    "    ie = IECore()\n",
    "    model_hp = 'intel/human-pose-estimation-0001/FP32/human-pose-estimation-0001'\n",
    "    net_hp  = ie.read_network(model=model_hp+'.xml', weights=model_hp+'.bin')\n",
    "    input_name_hp   = next(iter(net_hp.inputs))             # Input blob name \"data\"\n",
    "    input_shape_hp  = net_hp.inputs[input_name_hp].shape    # [1,3,256,456]\n",
    "    PAF_blobName    = list(net_hp.outputs.keys())[0]        # 'Mconv7_stage2_L1'\n",
    "    HM_blobName     = list(net_hp.outputs.keys())[1]        # 'Mconv7_stage2_L2'\n",
    "    PAF_shape       = net_hp.outputs[PAF_blobName].shape    #  [1,38,32,57] \n",
    "    HM_shape        = net_hp.outputs[HM_blobName].shape     #  [1,19,32,57]\n",
    "    exec_net_hp     = ie.load_network(net_hp, 'CPU')\n",
    "\n",
    "    # Open a USB webcam\n",
    "    #cam = cv2.VideoCapture(0)\n",
    "    cam = cv2.VideoCapture(video_path)\n",
    "    #cam.set(cv2.CAP_PROP_CONVERT_RGB, 0.0)\n",
    "    if cam.isOpened()==False:\n",
    "        print('Failed to open the input movie file (or a webCam)')\n",
    "        sys.exit(-1)\n",
    "\n",
    "    while cv2.waitKey(1) != 27:     # 27 == ESC\n",
    "\n",
    "        ret, img = cam.read()\n",
    "        if ret==False:\n",
    "            return 0\n",
    "\n",
    "        inblob = cv2.resize(img, (input_shape_hp[3], input_shape_hp[2]))    # 3=Width, 2=Height\n",
    "        inblob = inblob.transpose((2, 0, 1))                                # Change data layout from HWC to CHW\n",
    "        inblob = inblob.reshape(input_shape_hp)\n",
    "\n",
    "        res_hp = exec_net_hp.infer(inputs={input_name_hp: inblob})          # Infer poses\n",
    "\n",
    "        heatmaps = res_hp[HM_blobName ][0]\n",
    "        PAFs     = res_hp[PAF_blobName][0]\n",
    "        people = extract_poses(heatmaps[:-1], PAFs, 4)                      # Construct poses from HMs and PAFs\n",
    "\n",
    "        renderPeople(img, people, 4, 0.2)\n",
    "        #cv2.imshow('Result', img)\n",
    "        \n",
    "        clear_output(wait=True)\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        f = io.BytesIO()\n",
    "        PIL.Image.fromarray(img).save(f, 'jpeg')\n",
    "        IPython.display.display(IPython.display.Image(data=f.getvalue()))\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main(\"people.264\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
